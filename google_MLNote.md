## 问题构建（Framing）：  

### 什么是（监督式）机器学习：  

* 通过学习*如何组合输入信息*来对从未见过的数据做出有用的**预测**。  
  * Labels（标签）：指我们要预测的真实事物：**y**
    * 基本线性回归中的y变量。	  
  * Features（特征）：表示数据的方式，指用于描述数据的输入变量：**$x_i$**。
    * 基本线性回归中的${x_1, x_2,x_3,...,x_n}$。 
  * Sample(样本)：指数据的特定实例（为一个矢量）：**x**.
    * 有标签：具有{特征，标签}：（**x**，**y**）：用于训练模型。
    * 无标签：具有{特征，？}：（**x**，？）：用于对新数据做出预测。
  * Model（模型）：将样本映射到预测标签：**y'**
    * **执行预测的工具**。通过从数据债学习规律这一过程来尝试创建模型。
    * 由模型的内部参数定义，这些内部参数值是通过学习得到的。

## 深入了解机器学习（Descending into ML）：  

### **线性回归**：是一种找到最合适一组点的直线或超平面的方法。

$$ y = wx +b $$  

$w$为权重矢量（直线斜率）。$b$为偏差。

误差（loss ）（针对单个样本，采用模型的预测结果与真实值之间的方差）：  

给定样本的 **L2 损失**也称为平方误差

​	= 预测值和标签值之差的平方

​	= (观察值 - 预测值)2

​	= (y - y')2  

![](111.PNG)

**定义数据集上的 L2 损失**(方差)

$$L_2Loss = \sum_{(x,y)\in D} (y - prediction(x))^2$$

*不是专注于减少某一个样本，而是着眼于最大限度地减少整个数据集的误差。*

### 训练与损失  

**训练**模型表示通过有标签的样本来学习（确定）所有权重和偏差的理想值。在监督学习中，ML通过以下方式构建模型：  *检查多个样本并尝试找出最大限度地减少损失的模型----**经验风险最小化**。*

**损失**是对错误预测的惩罚。他是一个*数值*，表示对于单个样本而言模型预测的准确程度。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。

常见损失（平方损失）：  

**均方误差** (**MSE**) 指的是每个样本的*平均平方损失*。要计算 MSE，需要求出各个样本的所有平方损失之和，然后除以样本数量：

 $$MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2$$

其中，

* $(x,y)$指的是样本，其中

  * $x$指的是模型进行预测时使用的特征集（例如，温度、年龄和交配成功率）。
  * $y$指的是样本的标签（例如，每分钟的鸣叫次数）（也就是真实值）。

* $prediction(x)$指的是权重和偏差与特征集 $x$结合的函数（使用模型预测得到的值）。

* $D$指的是包含多个有标签样本（即 $(x,y)$）的数据集。

* $N$指的是 $D$ 中的样本数量。

  虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。

## 降低损失（Reducing Loss）：  

迭代方法是一种广泛用于降低损失的方法，使用起来也简单有效。  

### 如何降低损失？  

梯度：与模型参数相关的误差函数的导数。

- $(y - y')2$ 相对于权重和偏差的导数可让我们了解指定样本的损失变化情况

- - 易于计算且为凸形

- 因此，我们在能够尽可能降低损失的方向上反复采取小步

- - 我们将这些小步称为**梯度步长**（但它们实际上是负梯度步长）
  - 这种优化策略称为**梯度下降法**  

#### 梯度下降法示意图：  

![](2.PNG)

### 权重初始化:

- 对于凸形问题，权重可以从任何位置开始（比如，所有值为 0 的位置）

- - 凸形：想象一个碗的形状
  - 只有一个最低点

- 预示：不适用于神经网络

- - 非凸形：想象一个蛋托的形状
  - 有多个最低点
  - 很大程度上取决于初始值

### SGD 和小批量梯度下降法

- 可以在每步上计算整个数据集的梯度，但事实证明没有必要这样做

- 计算小型数据样本的梯度效果很好

- - 每一步抽取一个新的随机样本

- **随机梯度下降法**：一次抽取一个样本

- **小批量梯度下降法**：每批包含 10-1000 个样本

- - 损失和梯度在整批范围内达到平衡

*通常，可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已**收敛**。*  

**要点：**

在训练机器学习模型时，首先对权重和偏差进行初始猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止。

---------------------------------------------------

### 降低损失 (Reducing Loss)：梯度下降法:  

![1531308498244](../ML_note/3)

凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处，也可以称这个最低点为**全局最小值**。

但是，通过计算整个数据集中 w1 每个可能值的损失函数来找到收敛点这种方法效率太低。

因此可以研究一种更好的机制，这种机制在机器学习领域非常热门，称为**梯度下降法**。  

**梯度**是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。   

梯度是一个矢量，因此具有以下两个特征：

- 方向
- 大小

梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着**负梯度**的方向走一步，以便尽快降低损失。  

### 降低损失 (Reducing Loss)：学习速率:  

梯度矢量具有方向和大小。梯度下降法算法用梯度乘以一个称为**学习速率**（有时也称为**步长**）的标量，以确定下一个点的位置。   

每个回归问题都存在一个[金发姑娘](https://wikipedia.org/wiki/Goldilocks_principle)学习速率。“金发姑娘”值与损失函数的平坦程度相关。如果您知道损失函数的梯度较小，则可以放心地试着采用更大的学习速率，以补偿较小的梯度并获得更大的步长 。  

-------------------------------------------------------------------

一维空间中的理想学习速率是 $\frac1{f(x)″}$（f(x) 对 x 的二阶导数的倒数）。

二维或多维空间中的理想学习速率是[海森矩阵](https://wikipedia.org/wiki/Hessian_matrix)（由二阶偏导数组成的矩阵）的倒数。

广义凸函数的情况则更为复杂。    

---------------------------------

### 降低损失 (Reducing Loss)：随机梯度下降法:  

* **批量（batch）**指的是用于在单次迭代中计算梯度的样本总数。   

* 包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。 

* 通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。

*  **随机梯度下降法** (**SGD**) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。 

* **小批量随机梯度下降法**（**小批量 SGD**）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含**10-1000**个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。 

     

--------------------------------------

**有适用于模型调整的标准启发法吗？**
这是一个常见的问题。简短的答案是，**不同超参数的效果取决于数据**。因此，不存在必须遵循的规则，您需要对自己的数据进行测试。

即便如此，我们仍在下面列出了几条可为您提供指导的经验法则：

* 训练误差应该稳步减小，刚开始是急剧减小，最终应随着训练收敛达到平稳状态。
* 如果训练尚未收敛，尝试运行更长的时间。
* 如果训练误差减小但速度过慢，则提高学习速率也许有助于加快其减小速度。
  * 但有时如果学习速率过高，训练误差的减小速度反而会变慢。
* 如果训练误差变化很大，尝试降低学习速率。
  * 较低的学习速率和较大的步数/较大的批量大小通常是不错的组合。*
* 批量大小过小也会导致不稳定情况。不妨先尝试 100 或 1000 等较大的值，然后逐渐减小值的大小，直到出现性能降低的情况。

重申一下，切勿严格遵循这些经验法则，因为效果取决于数据。请始终进行试验和验证。  

--------------------------------------------------------------------------------------

### 识别离群值：  

在源数据中可能会存在一些异常数据影响模型的预测结果，可以称这些数据为**离群值**，也可说是偏离了源数据分布的一些值。

![1531381764087](../ML_note/5)

如上图右侧的个别点。由于点相对数量较小，故判断他们为离群值。

可以通过将这些离群值设置为相对合理的最小值或最大值来改进模型拟合情况。

例如初始情况，rooms_per_person的范围为0-8.右侧有几个离群值点：

![1531382069476](../ML_note/6)

通过下述rooms_per_person的直方图所示，发现大多数值都小于5，故将值截取为5.

![1531382088034](../ML_note/7)

在截取为5后的结果：

![1531382431766](../ML_note/8)

会发现少了那些离群值的影响，最后的数据分布看起来更加容易通过我们的模型来进行拟合。  

------------------------------------------------------------------------------

## 泛化（Generalization）：过拟合的风险  

**泛化** 指的是模型很好地拟合以前从没有见过的新数据（从用于创建该模型的同一分布中提取）的能力。也就是对测试数据的预测能力。      

**过拟合**模型在训练过程中产生的损失很低，但在预测新数据方面的表现却非常糟糕。  

*机器学习的基本冲突是适当拟合我们的数据，但也要尽可能简单地拟合数据*。  





### 概览：    

![1531466643654](12)

- 目标：针对从真实概率分布（已隐藏）中抽取的新数据做出良好预测。

- 问题：我们无法了解事实。

- - 我们只能从训练集中抽样。

- 如果模型 h 在拟合我们的当前样本方面表现良好，那么我们如何相信该模型会对其他新样本做出良好预测呢？   

### 我们如何得知自己的模型是否出色？

- 理论上：

- - 有趣的领域：泛化理论
  - 基于衡量模型简单性/复杂性的想法

- 直觉：奥卡姆剃刀定律的形式化

- - **模型越简单，良好的实证结果就越有可能不仅仅基于样本的特性。**    

奥卡姆剃刀：模型应该尽可能简单。  

**泛化边界**：根据以下因素，以统计的方式描述模型泛化到新数据的能力：

- 模型的复杂程度
- 模型在处理训练数据方面的表现

* 经验：
  * 提问：我们的模型能否在新的数据样本上表现良好？
  * 评估：获取新的数据样本 - 称为测试集
  * **一般来说，在测试集上表现是否良好是衡量能否在新数据上表现良好的有用指标**，前提是：
    * 如果测试集足够大
    * 如果我们不反复使用相同的测试集来作假  

数据集分成两个子集：

- **训练集** - 用于训练模型的子集。
- **测试集** - 用于测试模型的子集。

### 机器学习细则

以上所有情况的三项基本假设：

1. 我们从分布中随机抽取**独立同分布 (i.i.d.)** 的样本（i.i.d是表示变量随机性的一种方式），不以任何方式主动产生偏差。
2. 分布是**平稳的**：分布在数据集内不会随时间发生变化
3. 我们始终从**同一分布**中抽取样本：包括训练集、验证集和测试集

### 总结：

- 如果某个模型尝试紧密拟合训练数据，但却不能很好地泛化到新数据，就会发生过拟合。
- 如果不符合监督式机器学习的关键假设，那么我们将失去对新数据进行预测这项能力的重要理论保证。

--------------------------------------------------------------

## 训练集和测试集（Training and Test Sets）：拆分数据：  

当只有一个数据集时，可以通过拆分数据集来得到**训练集**和**测试集**。这两个数据集相互独立。  并且先通过随机化，再拆分。

![1531466799903](13)

* 训练集规模越大，模型的训练效果越好。
* 测试集规模越大，对评估指标的信心越充足，置信区间就越窄。
* 数据集规模较小，需要**交叉验证**等较为复杂的操作。  

### 如果我们只有一个数据集，该怎么办？

- 分成两个数据集：

- - 训练集
  - 测试集

- 典型陷阱：**请勿对测试数据进行训练**

- - 损失低得令人惊讶？
  - 在庆祝之前，请检查您是否不小心对测试数据进行了训练  

确保您的测试集满足以下两个条件：

- 规模足够大，可产生具有统计意义的结果。
- 能代表整个数据集。换言之，挑选的测试集的特征应该与训练集的特征相同。

------------------------------------------------------------------

## 验证（Validation）：检查你的直觉

在进行多轮超参数调整时，仅适用两类数据（训练集、测试集）可能不太够。  

下图为两种数据集可能的工作流程：

![1531556979832](222)

**“调整模型”**指的是调整您可以想到的关于模型的任何方面，从更改学习速率、添加或移除特征，到从头开始设计全新模型。 

这样的话，可能会对测试数据进行了过拟合。因此需要再划分数据集划分出一个新的部分---**验证集**。  

![1531557138804](12345.png)

使用**验证集**评估训练集的效果。然后，在模型“通过”验证集之后，使用测试集再次检查评估结果。 

**使用验证集中的数据来调整模型，直到在验证集上获得最佳效果的模型，才使用测试集来确认模型的效果。  并确保测试数据上得到的结果符合验证数据得到的结果。如果不符合，则说明对验证集进行了过拟合。**

![1531557626274](345.png)



![](图1-1531636429143.png)

 

---------------------------------------

## 表示（Representation）：特征工程  

*机器学习模型不能直接看到、听到或感知输入样本。必须自己创建数据**表示**，为模型提供有用的信号来了解数据的关键特性。*---------为了训练模型，必须选择最能代表数据的特征集。  

> 从各种各样的数据集中提取数据，将输入的数据映射为实用的机器学习特征，创建特征向量，并选择出合适的特征，处理离群值。  

### 从原始数据到特征：  

具体的原理是将左侧矢量的每个部分映射到右侧**特征矢量**（组成数据集中样本的浮点值集）的一个或多个字段。  

![1531707519341](456.png)

**特征工程**----从原始数据创建特征的过程，将原始数据转换为特征矢量。

* 如果数据为一个实值，则可以直接复制这个实值到特征向量。 
* 如果数据为一个字符串，则使用独热编码（one_hot_encoding)按照字典的形式将字符串映射为{0, ...,V-1} 内的某个整数。  
  * 首先为需要表示的所有特征的字符串定义一个词汇表。
  * 再使用该词汇表创建一个**独热编码**，用于将指定字符串表示为二元矢量。在改矢量中（与指定的字符串值相对应）：  
    * 只有一个元素设为1.
    * 其他所有元素均设为0.  
    * 该矢量的长度等于词汇表中的元素数。

![1531707721824](124354.png)  

### 映射分类（枚举）值:  

分类特征具有一组离散的可能值时，也就是说某个值可能属于多个类。那么就可以将分类特征编码为枚举类型或表示不同值的整数离散集。在机器学习中，通常将每个分类特征表示为单独的布尔值。

### 良好特征具备的特性：  

* 避免使用很少使用的离散特征值，大量离散值相同的样本可以让模型有机会了解不同设置中的特征，从而判断何时可以对标签很好地做出预测。特征值应以非零值的形式在数据集中多次出现。

  `错误：my_device_id:8SK982ZZ1242Z`

  `正确：device_model:galaxy_s6`  

* 特征应该具有清晰明确的含义：  

  `正确：user_age:23`

  `错误：user_age:123456789`

* 特征不应使用“神奇”的值（使用额外的布尔值特征），良好的浮点特征不包含超出范围的异常断点或“神奇”的值：  

  `错误：watch_time: -1.0`

  `正确：watch_time: 1.023`

  `正确：atch_time_is_defined: 1.0`  

  * 为解决神奇值的问题，需将该特征转换为两个特征：
    - 一个特征只存储质量评分，不含神奇值。
    - 一个特征存储布尔值，表示是否提供了 `watch_time`。为该布尔值特征指定一个名称，例如 `is_watch_time`。

* 特征的定义不应随着时间发生变化（数据的平稳性）：  

  `正确：city_id:"br/sao_paulo"`

  `错误：inferred_city_cluster_id:219`  

* 分布不应包含离谱的离群值（为特征设置上限或这换特征。理想情况下，所以特征都要转换为相似范围，例如（-1,1）或（0,5）。

### 分箱技巧：    

![1531708470026](435465.png)

- 如上图所示，按照整数分箱后拥有了11个不同的布尔值特征（`LatitudeBin1`、`LatitudeBin2`、…、`LatitudeBin11`），而不是一个浮点特征。 
- 这样可以将部分非线性关系映射到模型中。
- 创建若干个布尔值箱，每个箱映射到新的唯一特征。
- 允许模型为每个箱拟合不同的值。
- 分箱以后，模型就可以为每个维度学习完全不同的权重。
- *可以按照分位数分箱，可以确保每个桶中的样本数量是相等的。按分位数分箱完全无需担心离群值。*

### 良好习惯

**了解数据**：

- **可视化**：绘制直方图过散点图，以及各种排名指标将数据只管的显示出来。从最普遍到最不普遍排列。
- **调试**：进行各种数据错误测试检查。重复样本？缺少值？离群值？数据与信息中心一致？训练数据与验证数据相似？
- **监控**：监控一段时间内的数据。特征分位数、样本数量随着时间推移有无变化？    

遵循以下规则：

- 记住您预期的数据状态。
- 确认数据是否满足这些预期（或者您可以解释为何数据不满足预期）。
- 仔细检查训练数据是否与其他来源（例如信息中心）的数据一致。

像处理任何任务关键型代码一样谨慎处理您的数据。良好的机器学习依赖于良好的数据。

### 清理数据：  

* **缩放特征值** ：指将浮点特征值从自然范围（例如 100 到 900）转换为标准范围（例如 0 到 1 或 -1 到 +1）。如果某个特征集只包含一个特征，则缩放可以提供的实际好处微乎其微或根本没有。不过，如果特征集包含多个特征，则缩放特征可以带来以下优势：
  - 帮助梯度下降法更快速地收敛。
  - 帮助避免“NaN 陷阱”。在这种陷阱中，模型中的一个数值变成 [NaN](https://wikipedia.org/wiki/NaN)（例如，当某个值在训练期间超出浮点精确率限制时），并且模型中的所有其他数值最终也会因数学运算而变成 NaN。
  - 帮助模型为每个特征确定合适的权重。如果没有进行特征缩放，则模型会对范围较大的特征投入过多精力

> 要缩放数字数据，一种显而易见的方法是将$ [最小值，最大值]$ 以线性方式映射到较小的范围，例如 [-1，+1]。 
>
> 另一种热门的缩放策略是计算每个值的 Z 得分。Z 得分与距离均值mean的标准偏差数stddev相关。换而言之：
>
> $$scaled value = (value - mean) / stddev.$$
>
> 使用 Z 得分进行缩放意味着，大多数缩放后的值将介于 -3 和 +3 之间，而少量值将略高于或低于该范围。   

### 处理极端离群值：  

* 对每个值取对数。  
* 将特征值限制到一定的范围。  

### 清查：  

在现实生活中，数据集中的很多样本是不可靠的。原因有：  

* **遗漏值。** 例如，有人忘记为某个房屋的年龄输入值。
* **重复样本。** 例如，服务器错误地将同一条记录上传了两次。
* **不良标签。** 例如，有人错误地将一颗橡树的图片标记为枫树。
* **不良特征值。** 例如，有人输入了多余的位数，或者温度计被遗落在太阳底下。  

一旦检测到存在这些问题，通常需要将相应样本从数据集中移除，从而“修正”不良样本。  

除了检测各个不良样本之外，您还必须检测集合中的不良数据。直方图是一种用于可视化集合中数据的很好机制。此外，收集如下统计信息也会有所帮助：

- 最大值和最小值
- 均值和中间值
- 标准偏差